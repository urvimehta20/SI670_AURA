{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FVC Deepfake Detection: Complete Pipeline Journey\n",
    "\n",
    "**From Raw ZIP Files to Production-Ready Models**\n",
    "\n",
    "This notebook demonstrates the complete end-to-end machine learning pipeline for deepfake video detection, showcasing:\n",
    "- Data extraction and exploration\n",
    "- Augmentation strategy and rationale\n",
    "- Handcrafted feature engineering\n",
    "- Video scaling and preprocessing\n",
    "- Model training with MLOps infrastructure\n",
    "- Experiment tracking with MLflow\n",
    "- Analytics with DuckDB\n",
    "- Airflow orchestration\n",
    "\n",
    "**Target Audience**: ML Engineers, Data Scientists, Researchers\n",
    "**Level**: Production-Grade Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Project Setup & Requirements](#1-project-setup--requirements)\n",
    "2. [Data Extraction from ZIP Archives](#2-data-extraction-from-zip-archives)\n",
    "3. [Data Exploration & Analysis](#3-data-exploration--analysis)\n",
    "4. [Stage 1: Video Augmentation](#4-stage-1-video-augmentation)\n",
    "5. [Stage 2: Handcrafted Feature Extraction](#5-stage-2-handcrafted-feature-extraction)\n",
    "6. [Stage 3: Video Scaling](#6-stage-3-video-scaling)\n",
    "7. [Stage 4: Scaled Feature Extraction](#7-stage-4-scaled-feature-extraction)\n",
    "8. [Stage 5: Model Training](#8-stage-5-model-training)\n",
    "9. [MLOps Infrastructure](#9-mlops-infrastructure)\n",
    "10. [Analytics with DuckDB](#10-analytics-with-duckdb)\n",
    "11. [Airflow Orchestration](#11-airflow-orchestration)\n",
    "12. [Results & Insights](#12-results--insights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Project Setup & Requirements\n",
    "\n",
    "### Infrastructure Stack\n",
    "\n",
    "**Core ML Framework**:\n",
    "- PyTorch 2.0+ with CUDA support\n",
    "- torchvision for video models\n",
    "- timm for Vision Transformers\n",
    "\n",
    "**Data Processing**:\n",
    "- Polars 0.19+ (10-100x faster than pandas)\n",
    "- PyArrow 14+ for columnar storage (Arrow/Parquet)\n",
    "- DuckDB 0.9+ for analytical queries\n",
    "\n",
    "**MLOps & Orchestration**:\n",
    "- MLflow for experiment tracking\n",
    "- Apache Airflow for pipeline orchestration\n",
    "- Custom ExperimentTracker for run management\n",
    "\n",
    "**Video Processing**:\n",
    "- PyAV for efficient video I/O\n",
    "- OpenCV for feature extraction\n",
    "- FFmpeg/ffprobe for codec analysis\n",
    "\n",
    "**Feature Engineering**:\n",
    "- NumPy for signal processing\n",
    "- scikit-image for image analysis\n",
    "- Custom handcrafted feature extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML, Video\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().absolute().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import project modules\n",
    "from lib.data.index import build_video_index, FVCConfig\n",
    "from lib.data.loading import load_metadata, filter_existing_videos\n",
    "from lib.augmentation.pipeline import stage1_augment_videos\n",
    "from lib.features.pipeline import stage2_extract_features\n",
    "from lib.scaling.pipeline import stage3_scale_videos\n",
    "from lib.features.scaled import stage4_extract_scaled_features\n",
    "from lib.training.pipeline import stage5_train_models\n",
    "from lib.utils.duckdb_analytics import DuckDBAnalytics\n",
    "from lib.mlops.mlflow_tracker import create_mlflow_tracker\n",
    "from lib.mlops.config import RunConfig, ExperimentTracker\n",
    "from lib.utils.paths import load_metadata_flexible\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"\\n✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Requirements\n",
    "\n",
    "Check that all required packages are installed and versions are compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import polars as pl\n",
    "import pyarrow as pa\n",
    "\n",
    "try:\n",
    "    import duckdb\n",
    "    print(f\"✓ DuckDB {duckdb.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"✗ DuckDB not installed\")\n",
    "\n",
    "try:\n",
    "    import mlflow\n",
    "    print(f\"✓ MLflow {mlflow.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"✗ MLflow not installed\")\n",
    "\n",
    "print(f\"✓ PyTorch {torch.__version__}\")\n",
    "print(f\"✓ torchvision {torchvision.__version__}\")\n",
    "print(f\"✓ Polars {pl.__version__}\")\n",
    "print(f\"✓ PyArrow {pa.__version__}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✓ CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"⚠ CUDA not available (CPU mode)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Extraction from ZIP Archives\n",
    "\n",
    "### Initial Data Structure\n",
    "\n",
    "The FVC dataset comes as password-protected ZIP archives:\n",
    "- `FVC1.zip`, `FVC2.zip`, `FVC3.zip`: Video files\n",
    "- `Metadata.zip`: CSV metadata files\n",
    "\n",
    "**Setup Process**:\n",
    "1. Extract videos from ZIP archives\n",
    "2. Copy metadata CSV files\n",
    "3. Build video index with comprehensive statistics\n",
    "4. Validate data integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for archive files\n",
    "archive_dir = project_root / \"archive\"\n",
    "print(f\"Archive directory: {archive_dir}\")\n",
    "print(f\"Archive directory exists: {archive_dir.exists()}\")\n",
    "\n",
    "if archive_dir.exists():\n",
    "    archive_files = list(archive_dir.glob(\"*.zip\"))\n",
    "    csv_files = list(archive_dir.glob(\"*.csv\"))\n",
    "    \n",
    "    print(f\"\\nFound {len(archive_files)} ZIP archives:\")\n",
    "    for f in archive_files:\n",
    "        print(f\"  - {f.name} ({f.stat().st_size / 1024**3:.2f} GB)\")\n",
    "    \n",
    "    print(f\"\\nFound {len(csv_files)} CSV metadata files:\")\n",
    "    for f in csv_files:\n",
    "        print(f\"  - {f.name}\")\n",
    "        \n",
    "    # Load and inspect metadata\n",
    "    if csv_files:\n",
    "        metadata_path = csv_files[0]\n",
    "        print(f\"\\nLoading metadata from: {metadata_path.name}\")\n",
    "        \n",
    "        # Use Polars for fast loading\n",
    "        df = pl.read_csv(metadata_path)\n",
    "        print(f\"\\nMetadata shape: {df.shape}\")\n",
    "        print(f\"\\nColumns: {df.columns}\")\n",
    "        print(f\"\\nFirst few rows:\")\n",
    "        display(df.head())\n",
    "        \n",
    "        # Basic statistics\n",
    "        if \"label\" in df.columns:\n",
    "            label_counts = df.group_by(\"label\").agg(pl.count().alias(\"count\"))\n",
    "            print(f\"\\nLabel distribution:\")\n",
    "            display(label_counts)\n",
    "else:\n",
    "    print(\"⚠ Archive directory not found. Run setup_fvc_dataset.py first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Setup Script\n",
    "\n",
    "The `setup_fvc_dataset.py` script handles:\n",
    "- Password-protected ZIP extraction\n",
    "- Video organization into `fvc/videos/FVC[1-3]/`\n",
    "- Metadata file copying\n",
    "- Video index generation with ffprobe statistics\n",
    "\n",
    "**Key Features**:\n",
    "- Handles disk space issues gracefully\n",
    "- Validates extracted files\n",
    "- Generates comprehensive video statistics (fps, duration, codec, bitrate, resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if data is already set up\n",
    "data_dir = project_root / \"data\"\n",
    "videos_dir = project_root / \"fvc\" / \"videos\"\n",
    "\n",
    "print(f\"Data directory: {data_dir.exists()}\")\n",
    "print(f\"Videos directory: {videos_dir.exists()}\")\n",
    "\n",
    "if videos_dir.exists():\n",
    "    video_folders = [d for d in videos_dir.iterdir() if d.is_dir()]\n",
    "    print(f\"\\nFound {len(video_folders)} video folders:\")\n",
    "    for folder in video_folders:\n",
    "        video_count = len(list(folder.glob(\"*.mp4\")))\n",
    "        print(f\"  - {folder.name}: {video_count} videos\")\n",
    "    \n",
    "    # Check for metadata index\n",
    "    metadata_index = data_dir / \"metadata\" / \"video_index.arrow\"\n",
    "    if metadata_index.exists():\n",
    "        print(f\"\\n✓ Video index found: {metadata_index}\")\n",
    "        index_df = pl.read_ipc(metadata_index)\n",
    "        print(f\"  Index shape: {index_df.shape}\")\n",
    "        print(f\"  Columns: {index_df.columns}\")\n",
    "    else:\n",
    "        print(f\"\\n⚠ Video index not found. Run: python src/setup_fvc_dataset.py\")\n",
    "else:\n",
    "    print(\"\\n⚠ Videos directory not found. Run: python src/setup_fvc_dataset.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Exploration & Analysis\n",
    "\n",
    "### Why Data Exploration Matters\n",
    "\n",
    "Understanding the dataset is crucial for:\n",
    "- **Augmentation Strategy**: What transformations preserve authenticity?\n",
    "- **Feature Engineering**: What signals distinguish real vs fake?\n",
    "- **Model Architecture**: What input dimensions and temporal lengths?\n",
    "- **Preprocessing**: What scaling/normalization is needed?\n",
    "\n",
    "### Key Questions to Answer\n",
    "1. What is the class distribution? (affects sampling strategy)\n",
    "2. What are video resolutions? (affects memory and scaling)\n",
    "3. What are video durations? (affects frame sampling)\n",
    "4. What codecs are used? (affects decoding strategy)\n",
    "5. Are there duplicates? (affects train/test split strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata for exploration\n",
    "metadata_paths = [\n",
    "    project_root / \"data\" / \"metadata\" / \"video_index.arrow\",\n",
    "    project_root / \"archive\" / \"FVC.csv\",\n",
    "    project_root / \"archive\" / \"FVC_dup.csv\",\n",
    "]\n",
    "\n",
    "df = None\n",
    "for path in metadata_paths:\n",
    "    if path.exists():\n",
    "        print(f\"Loading from: {path}\")\n",
    "        df = load_metadata_flexible(str(path))\n",
    "        if df is not None and df.height > 0:\n",
    "            break\n",
    "\n",
    "if df is not None:\n",
    "    print(f\"\\n✓ Loaded {df.height} videos\")\n",
    "    print(f\"Columns: {df.columns}\")\n",
    "    \n",
    "    # Class distribution\n",
    "    if \"label\" in df.columns:\n",
    "        label_dist = df.group_by(\"label\").agg(\n",
    "            pl.count().alias(\"count\"),\n",
    "            (pl.count() / pl.len() * 100).alias(\"percentage\")\n",
    "        ).sort(\"label\")\n",
    "        print(\"\\nClass Distribution:\")\n",
    "        display(label_dist)\n",
    "    \n",
    "    # Video statistics (if available)\n",
    "    stat_cols = [\"width\", \"height\", \"duration\", \"fps\", \"bitrate\"]\n",
    "    available_stats = [c for c in stat_cols if c in df.columns]\n",
    "    \n",
    "    if available_stats:\n",
    "        print(\"\\nVideo Statistics:\")\n",
    "        stats = df.select(available_stats).describe()\n",
    "        display(stats)\n",
    "        \n",
    "        # Visualizations\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        if \"width\" in df.columns and \"height\" in df.columns:\n",
    "            axes[0, 0].scatter(df[\"width\"], df[\"height\"], alpha=0.5)\n",
    "            axes[0, 0].set_xlabel(\"Width\")\n",
    "            axes[0, 0].set_ylabel(\"Height\")\n",
    "            axes[0, 0].set_title(\"Video Resolution Distribution\")\n",
    "        \n",
    "        if \"duration\" in df.columns:\n",
    "            axes[0, 1].hist(df[\"duration\"].to_numpy(), bins=50, edgecolor='black')\n",
    "            axes[0, 1].set_xlabel(\"Duration (seconds)\")\n",
    "            axes[0, 1].set_ylabel(\"Count\")\n",
    "            axes[0, 1].set_title(\"Video Duration Distribution\")\n",
    "        \n",
    "        if \"fps\" in df.columns:\n",
    "            axes[1, 0].hist(df[\"fps\"].to_numpy(), bins=30, edgecolor='black')\n",
    "            axes[1, 0].set_xlabel(\"FPS\")\n",
    "            axes[1, 0].set_ylabel(\"Count\")\n",
    "            axes[1, 0].set_title(\"Frame Rate Distribution\")\n",
    "        \n",
    "        if \"label\" in df.columns:\n",
    "            label_counts = df.group_by(\"label\").agg(pl.count()).sort(\"label\")\n",
    "            axes[1, 1].bar(label_counts[\"label\"].to_list(), label_counts[\"count\"].to_list())\n",
    "            axes[1, 1].set_xlabel(\"Label\")\n",
    "            axes[1, 1].set_ylabel(\"Count\")\n",
    "            axes[1, 1].set_title(\"Class Distribution\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"⚠ No metadata found. Run data setup first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights from Data Exploration\n",
    "\n",
    "**Key Findings** (based on typical deepfake datasets):\n",
    "\n",
    "1. **Resolution Diversity**: Videos range from 240p to 1080p+\n",
    "   - **Decision**: Scale to fixed 256x256 max dimension (letterboxing)\n",
    "   - **Rationale**: Consistent batch dimensions, memory efficiency, maintains aspect ratio\n",
    "\n",
    "2. **Duration Variability**: Videos range from 1-60+ seconds\n",
    "   - **Decision**: Uniform frame sampling (6-16 frames)\n",
    "   - **Rationale**: Fixed temporal length for batch processing\n",
    "\n",
    "3. **Class Imbalance**: May have 60/40 or 70/30 real/fake split\n",
    "   - **Decision**: Stratified k-fold CV, balanced batch sampling\n",
    "   - **Rationale**: Prevents bias toward majority class\n",
    "\n",
    "4. **Codec Diversity**: H.264, H.265, VP9, etc.\n",
    "   - **Decision**: PyAV for universal decoding, codec-aware feature extraction\n",
    "   - **Rationale**: Robust handling of different codecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stage 1: Video Augmentation\n",
    "\n",
    "### Why Augmentation?\n",
    "\n",
    "**Problem**: Limited dataset size (typically 200-500 videos)\n",
    "**Solution**: Generate augmented versions to increase diversity\n",
    "\n",
    "### Augmentation Strategy\n",
    "\n",
    "**Spatial Augmentations** (per-frame):\n",
    "1. **Rotation** (±10°): Simulates camera angle variation\n",
    "2. **Horizontal Flip**: Doubles dataset, preserves temporal structure\n",
    "3. **Brightness/Contrast/Saturation**: Handles lighting variations\n",
    "4. **Gaussian Noise**: Adds robustness to compression artifacts\n",
    "5. **Gaussian Blur**: Simulates motion blur, low-quality captures\n",
    "6. **Affine Transformations**: Translation, scale, shear\n",
    "7. **Elastic Transform**: Simulates non-rigid deformations\n",
    "8. **Cutout**: Random erasing for occlusion robustness\n",
    "\n",
    "**Temporal Augmentations** (sequence-level):\n",
    "1. **Frame Dropping** (up to 25%): Handles variable frame rates\n",
    "2. **Frame Duplication**: Slow motion effect\n",
    "3. **Temporal Reversal**: Time-reversed videos\n",
    "\n",
    "### Implementation: Pre-Generated vs On-the-Fly\n",
    "\n",
    "**Why Pre-Generated?**\n",
    "- **Reproducibility**: Same augmentations across runs\n",
    "- **Speed**: No augmentation overhead during training\n",
    "- **Caching**: Can store on disk, share across experiments\n",
    "- **Memory**: Frame-by-frame decoding (50x memory reduction)\n",
    "\n",
    "**Trade-offs**:\n",
    "- Disk space: 10x dataset size (mitigated by scaling)\n",
    "- Initial processing time (one-time cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1: Augmentation\n",
    "# This generates 10 augmented versions per video (configurable)\n",
    "\n",
    "print(\"Stage 1: Video Augmentation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if augmentation already done\n",
    "augmented_metadata = project_root / \"data\" / \"augmented_videos\" / \"augmented_metadata.arrow\"\n",
    "\n",
    "if augmented_metadata.exists():\n",
    "    print(f\"✓ Augmentation already completed\")\n",
    "    aug_df = pl.read_ipc(augmented_metadata)\n",
    "    print(f\"  Total videos: {aug_df.height}\")\n",
    "    \n",
    "    # Count original vs augmented\n",
    "    if \"is_augmented\" in aug_df.columns:\n",
    "        orig_count = aug_df.filter(pl.col(\"is_augmented\") == False).height\n",
    "        aug_count = aug_df.filter(pl.col(\"is_augmented\") == True).height\n",
    "        print(f\"  Original videos: {orig_count}\")\n",
    "        print(f\"  Augmented videos: {aug_count}\")\n",
    "        print(f\"  Augmentation ratio: {aug_count / orig_count:.1f}x\")\n",
    "    \n",
    "    # Show sample augmented video paths\n",
    "    if \"video_path\" in aug_df.columns:\n",
    "        sample_paths = aug_df.head(5)[\"video_path\"].to_list()\n",
    "        print(f\"\\nSample augmented videos:\")\n",
    "        for path in sample_paths[:3]:\n",
    "            print(f\"  - {Path(path).name}\")\n",
    "else:\n",
    "    print(\"⚠ Augmentation not completed yet.\")\n",
    "    print(\"\\nTo run augmentation:\")\n",
    "    print(\"```python\")\n",
    "    print(\"from lib.augmentation.pipeline import stage1_augment_videos\")\n",
    "    print(\"\")\n",
    "    print(\"augmented_df = stage1_augment_videos(\")\n",
    "    print(\"    project_root=str(project_root),\")\n",
    "    print(\"    num_augmentations=10,  # 10 augmented versions per video\")\n",
    "    print(\"    output_dir='data/augmented_videos',\")\n",
    "    print(\"    delete_existing=False\")\n",
    "    print(\")\")\n",
    "    print(\"```\")\n",
    "    print(\"\\nOr use SLURM script:\")\n",
    "    print(\"```bash\")\n",
    "    print(\"sbatch src/scripts/slurm_stage1_augmentation.sh\")\n",
    "    print(\"```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation Implementation Details\n",
    "\n",
    "**Key Optimizations**:\n",
    "\n",
    "1. **Chunked Processing**: Videos processed in 250-frame chunks\n",
    "   - Prevents OOM for long videos\n",
    "   - Supports checkpointing/resuming\n",
    "\n",
    "2. **Frame-by-Frame Decoding**: Decode only needed frames\n",
    "   - Memory: ~37 MB per video (vs ~1.87 GB full load)\n",
    "   - 50x memory reduction\n",
    "\n",
    "3. **Deterministic Seeds**: Hash-based seeds per video\n",
    "   - Reproducible augmentations\n",
    "   - Same video → same augmentations\n",
    "\n",
    "4. **Incremental Metadata Writing**: Direct CSV writing\n",
    "   - No memory accumulation\n",
    "   - Constant memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stage 2: Handcrafted Feature Extraction\n",
    "\n",
    "### Why Handcrafted Features?\n",
    "\n",
    "**Rationale**:\n",
    "- **Interpretability**: Understand what signals the model uses\n",
    "- **Baseline Models**: Enable traditional ML (Logistic Regression, SVM)\n",
    "- **Complementary**: Works alongside deep learning features\n",
    "- **Fast Inference**: No GPU required for feature extraction\n",
    "\n",
    "### Feature Types\n",
    "\n",
    "**1. Noise Residual Energy** (3 features)\n",
    "- **Purpose**: Detect compression artifacts, manipulation traces\n",
    "- **Method**: High-pass filter (Laplacian) → energy statistics\n",
    "- **Rationale**: Deepfakes often introduce compression inconsistencies\n",
    "\n",
    "**2. DCT Statistics** (5 features)\n",
    "- **Purpose**: Capture frequency domain patterns\n",
    "- **Method**: 8x8 DCT blocks → DC/AC coefficient statistics\n",
    "- **Rationale**: Video codecs use DCT; artifacts show in frequency domain\n",
    "\n",
    "**3. Blur/Sharpness Metrics** (3 features)\n",
    "- **Purpose**: Detect unnatural sharpness/blur patterns\n",
    "- **Method**: Laplacian variance, gradient magnitude\n",
    "- **Rationale**: Face-swapping can create inconsistent sharpness\n",
    "\n",
    "**4. Block Boundary Inconsistency** (1 feature)\n",
    "- **Purpose**: Detect block-based compression artifacts\n",
    "- **Method**: Analyze block boundary discontinuities\n",
    "- **Rationale**: Double compression leaves traces\n",
    "\n",
    "**5. Codec Cues** (3 features)\n",
    "- **Purpose**: Extract codec metadata (if available)\n",
    "- **Method**: ffprobe analysis of codec parameters\n",
    "- **Rationale**: Different codecs used for real vs fake videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2: Handcrafted Feature Extraction\n",
    "print(\"Stage 2: Handcrafted Feature Extraction\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if features already extracted\n",
    "features_metadata = project_root / \"data\" / \"stage2\" / \"features_metadata.arrow\"\n",
    "\n",
    "if features_metadata.exists():\n",
    "    print(f\"✓ Features already extracted\")\n",
    "    feat_df = pl.read_ipc(features_metadata)\n",
    "    print(f\"  Total feature rows: {feat_df.height}\")\n",
    "    \n",
    "    # Show feature columns (excluding metadata)\n",
    "    feature_cols = [c for c in feat_df.columns if c not in [\"video_path\", \"label\", \"is_augmented\"]]\n",
    "    print(f\"  Number of features: {len(feature_cols)}\")\n",
    "    print(f\"  Feature names: {feature_cols[:10]}...\" if len(feature_cols) > 10 else f\"  Feature names: {feature_cols}\")\n",
    "    \n",
    "    # Show feature statistics\n",
    "    if feature_cols:\n",
    "        print(\"\\nFeature Statistics:\")\n",
    "        stats = feat_df.select(feature_cols[:5]).describe()  # First 5 features\n",
    "        display(stats)\n",
    "        \n",
    "        # Feature distribution visualization\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for idx, feat_name in enumerate(feature_cols[:6]):\n",
    "            if idx < len(axes):\n",
    "                feat_values = feat_df[feat_name].to_numpy()\n",
    "                axes[idx].hist(feat_values, bins=50, edgecolor='black')\n",
    "                axes[idx].set_title(f\"{feat_name}\")\n",
    "                axes[idx].set_xlabel(\"Value\")\n",
    "                axes[idx].set_ylabel(\"Frequency\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"⚠ Features not extracted yet.\")\n",
    "    print(\"\\nTo extract features:\")\n",
    "    print(\"```python\")\n",
    "    print(\"from lib.features.pipeline import stage2_extract_features\")\n",
    "    print(\"\")\n",
    "    print(\"features_df = stage2_extract_features(\")\n",
    "    print(\"    project_root=str(project_root),\")\n",
    "    print(\"    augmented_metadata_path='data/augmented_videos/augmented_metadata.arrow',\")\n",
    "    print(\"    output_dir='data/stage2',\")\n",
    "    print(\"    num_frames=50,  # Sample 50 frames per video\")\n",
    "    print(\"    delete_existing=False\")\n",
    "    print(\")\")\n",
    "    print(\"```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction Implementation\n",
    "\n",
    "**Sampling Strategy**:\n",
    "- **Adaptive Frame Sampling**: 10% of frames (min 5, max 50)\n",
    "- **Rationale**: Balance between coverage and computation\n",
    "- **Aggregation**: Mean, std, min, max across sampled frames\n",
    "\n",
    "**Storage Format**:\n",
    "- **NumPy Arrays**: `.npy` files per video\n",
    "- **Metadata**: Arrow/Parquet format (fast querying)\n",
    "- **Rationale**: Efficient storage, fast loading for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Stage 3: Video Scaling\n",
    "\n",
    "### Why Scale Videos?\n",
    "\n",
    "**Problem**: Variable resolutions (240p to 1080p+)\n",
    "- Inconsistent batch dimensions\n",
    "- Memory inefficiency\n",
    "- Model input requirements\n",
    "\n",
    "**Solution**: Scale to fixed max dimension (256px)\n",
    "- **Letterboxing**: Preserves aspect ratio\n",
    "- **Upscaling/Downscaling**: Both directions supported\n",
    "- **Consistent Dimensions**: Enables batch processing\n",
    "\n",
    "### Scaling Methods\n",
    "\n",
    "**1. Letterbox Resize** (default)\n",
    "- Simple bilinear interpolation\n",
    "- Fast, memory-efficient\n",
    "- Preserves aspect ratio\n",
    "\n",
    "**2. Autoencoder Scaling** (optional)\n",
    "- Uses pretrained VAE (Stable Diffusion)\n",
    "- Higher quality upscaling\n",
    "- More compute-intensive\n",
    "\n",
    "### Decision: Letterbox Resize\n",
    "- **Speed**: 100x faster than autoencoder\n",
    "- **Memory**: Lower GPU memory usage\n",
    "- **Quality**: Sufficient for deepfake detection\n",
    "- **Trade-off**: Slight quality loss acceptable for speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 3: Video Scaling\n",
    "print(\"Stage 3: Video Scaling\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if scaling already done\n",
    "scaled_metadata = project_root / \"data\" / \"stage3\" / \"scaled_metadata.arrow\"\n",
    "\n",
    "if scaled_metadata.exists():\n",
    "    print(f\"✓ Scaling already completed\")\n",
    "    scaled_df = pl.read_ipc(scaled_metadata)\n",
    "    print(f\"  Total scaled videos: {scaled_df.height}\")\n",
    "    \n",
    "    # Check scaling statistics\n",
    "    if \"is_upscaled\" in scaled_df.columns and \"is_downscaled\" in scaled_df.columns:\n",
    "        upscaled = scaled_df.filter(pl.col(\"is_upscaled\") == True).height\n",
    "        downscaled = scaled_df.filter(pl.col(\"is_downscaled\") == True).height\n",
    "        print(f\"  Upscaled videos: {upscaled}\")\n",
    "        print(f\"  Downscaled videos: {downscaled}\")\n",
    "    \n",
    "    # Show sample scaled video paths\n",
    "    if \"video_path\" in scaled_df.columns:\n",
    "        sample_paths = scaled_df.head(3)[\"video_path\"].to_list()\n",
    "        print(f\"\\nSample scaled videos:\")\n",
    "        for path in sample_paths:\n",
    "            print(f\"  - {Path(path).name}\")\n",
    "else:\n",
    "    print(\"⚠ Scaling not completed yet.\")\n",
    "    print(\"\\nTo scale videos:\")\n",
    "    print(\"```python\")\n",
    "    print(\"from lib.scaling.pipeline import stage3_scale_videos\")\n",
    "    print(\"\")\n",
    "    print(\"scaled_df = stage3_scale_videos(\")\n",
    "    print(\"    project_root=str(project_root),\")\n",
    "    print(\"    augmented_metadata_path='data/augmented_videos/augmented_metadata.arrow',\")\n",
    "    print(\"    output_dir='data/stage3',\")\n",
    "    print(\"    target_size=256,  # Max dimension (width or height)\")\n",
    "    print(\"    method='letterbox',  # or 'autoencoder'\")\n",
    "    print(\"    delete_existing=False\")\n",
    "    print(\")\")\n",
    "    print(\"```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Stage 4: Scaled Feature Extraction\n",
    "\n",
    "### Why Extract Features from Scaled Videos?\n",
    "\n",
    "**Rationale**:\n",
    "- **Different Resolution Context**: Features may behave differently at scaled resolution\n",
    "- **Additional Signals**: `is_upscaled`, `is_downscaled` flags\n",
    "- **Complementary Features**: Works alongside Stage 2 features\n",
    "\n",
    "**Same Feature Types as Stage 2**:\n",
    "- Noise residual\n",
    "- DCT statistics\n",
    "- Blur/sharpness\n",
    "- Boundary inconsistency\n",
    "- Codec cues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 4: Scaled Feature Extraction\n",
    "print(\"Stage 4: Scaled Feature Extraction\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if scaled features already extracted\n",
    "scaled_features_metadata = project_root / \"data\" / \"stage4\" / \"features_scaled_metadata.arrow\"\n",
    "\n",
    "if scaled_features_metadata.exists():\n",
    "    print(f\"✓ Scaled features already extracted\")\n",
    "    scaled_feat_df = pl.read_ipc(scaled_features_metadata)\n",
    "    print(f\"  Total scaled feature rows: {scaled_feat_df.height}\")\n",
    "    \n",
    "    # Compare with Stage 2 features\n",
    "    if features_metadata.exists():\n",
    "        feat_df = pl.read_ipc(features_metadata)\n",
    "        print(f\"  Stage 2 features: {feat_df.height} rows\")\n",
    "        print(f\"  Stage 4 features: {scaled_feat_df.height} rows\")\n",
    "else:\n",
    "    print(\"⚠ Scaled features not extracted yet.\")\n",
    "    print(\"\\nTo extract scaled features:\")\n",
    "    print(\"```python\")\n",
    "    print(\"from lib.features.scaled import stage4_extract_scaled_features\")\n",
    "    print(\"\")\n",
    "    print(\"scaled_features_df = stage4_extract_scaled_features(\")\n",
    "    print(\"    project_root=str(project_root),\")\n",
    "    print(\"    scaled_metadata_path='data/stage3/scaled_metadata.arrow',\")\n",
    "    print(\"    output_dir='data/stage4',\")\n",
    "    print(\"    num_frames=50,\")\n",
    "    print(\"    delete_existing=False\")\n",
    "    print(\")\")\n",
    "    print(\"```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Stage 5: Model Training\n",
    "\n",
    "### Model Architecture Overview\n",
    "\n",
    "We train **23 different models** across multiple categories:\n",
    "\n",
    "**Baseline Models** (Feature-based):\n",
    "- Logistic Regression (5a, 5alpha)\n",
    "- SVM (5b)\n",
    "- Gradient Boosting (5beta: XGBoost, LightGBM, CatBoost)\n",
    "\n",
    "**PyTorch CNNs**:\n",
    "- Naive 3D CNN (5c)\n",
    "- Pretrained Inception Video (5d)\n",
    "- Variable AR CNN (5e)\n",
    "\n",
    "**XGBoost + Deep Features**:\n",
    "- XGBoost with Inception/I3D/R2Plus1D/ViT features (5f-5j)\n",
    "\n",
    "**Vision Transformers**:\n",
    "- ViT-GRU (5k)\n",
    "- ViT-Transformer (5l)\n",
    "\n",
    "**Video Transformers**:\n",
    "- TimeSformer (5m)\n",
    "- ViViT (5n)\n",
    "\n",
    "**3D CNNs**:\n",
    "- I3D (5o)\n",
    "- R(2+1)D (5p)\n",
    "- X3D (5q)\n",
    "\n",
    "**SlowFast**:\n",
    "- SlowFast (5r)\n",
    "- SlowFast Attention (5s)\n",
    "- Multi-Scale SlowFast (5t)\n",
    "\n",
    "**Two-Stream**:\n",
    "- RGB + Optical Flow (5u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Configuration\n",
    "\n",
    "**Hyperparameter Strategy**:\n",
    "- **Grid Search**: Single combination per model (reduced from 5+ for efficiency)\n",
    "- **Learning Rates**: Different for backbone (5e-6) vs head (5e-4)\n",
    "- **Batch Sizes**: Memory-constrained (1-4 depending on model)\n",
    "- **Epochs**: 20-25 (with early stopping)\n",
    "\n",
    "**Regularization**:\n",
    "- **Weight Decay**: 1e-4 (L2 regularization)\n",
    "- **Dropout**: 0.5 in classification heads\n",
    "- **Early Stopping**: Patience=5 epochs\n",
    "\n",
    "**Optimization**:\n",
    "- **Optimizer**: Adam with default betas\n",
    "- **Mixed Precision**: AMP for memory efficiency\n",
    "- **Gradient Accumulation**: Dynamic based on batch size\n",
    "\n",
    "**Cross-Validation**:\n",
    "- **Stratified 5-Fold CV**: Ensures class balance in each fold\n",
    "- **Rationale**: Robust performance estimates, prevents overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 5: Model Training Overview\n",
    "print(\"Stage 5: Model Training\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check for trained models\n",
    "stage5_dir = project_root / \"data\" / \"stage5\"\n",
    "\n",
    "if stage5_dir.exists():\n",
    "    model_dirs = [d for d in stage5_dir.iterdir() if d.is_dir()]\n",
    "    print(f\"✓ Found {len(model_dirs)} trained model directories:\")\n",
    "    \n",
    "    for model_dir in sorted(model_dirs)[:10]:  # Show first 10\n",
    "        model_name = model_dir.name\n",
    "        \n",
    "        # Check for model files\n",
    "        model_files = list(model_dir.rglob(\"model.pt\")) + list(model_dir.rglob(\"model.joblib\"))\n",
    "        metrics_files = list(model_dir.rglob(\"metrics.json\"))\n",
    "        \n",
    "        if model_files:\n",
    "            print(f\"  ✓ {model_name}: {len(model_files)} model(s), {len(metrics_files)} metrics file(s)\")\n",
    "        else:\n",
    "            print(f\"  ⚠ {model_name}: No models found\")\n",
    "    \n",
    "    if len(model_dirs) > 10:\n",
    "        print(f\"  ... and {len(model_dirs) - 10} more\")\n",
    "else:\n",
    "    print(\"⚠ No trained models found.\")\n",
    "    print(\"\\nTo train models:\")\n",
    "    print(\"```python\")\n",
    "    print(\"from lib.training.pipeline import stage5_train_models\")\n",
    "    print(\"\")\n",
    "    print(\"results = stage5_train_models(\")\n",
    "    print(\"    project_root=str(project_root),\")\n",
    "    print(\"    scaled_metadata_path='data/stage3/scaled_metadata.arrow',\")\n",
    "    print(\"    features_stage2_path='data/stage2/features_metadata.arrow',\")\n",
    "    print(\"    features_stage4_path='data/stage4/features_scaled_metadata.arrow',\")\n",
    "    print(\"    model_types=['logistic_regression', 'svm', 'naive_cnn', ...],\")\n",
    "    print(\"    n_splits=5,  # 5-fold CV\")\n",
    "    print(\"    output_dir='data/stage5',\")\n",
    "    print(\"    use_mlflow=True,\")\n",
    "    print(\"    delete_existing=False\")\n",
    "    print(\")\")\n",
    "    print(\"```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. MLOps Infrastructure\n",
    "\n",
    "### Experiment Tracking with MLflow\n",
    "\n",
    "**Features**:\n",
    "- **Run Tracking**: Unique run IDs, timestamps, tags\n",
    "- **Parameter Logging**: Hyperparameters, configs\n",
    "- **Metrics Logging**: Training/validation metrics per epoch\n",
    "- **Artifact Storage**: Model checkpoints, plots, configs\n",
    "- **Model Registry**: Versioned model storage\n",
    "\n",
    "**Integration**:\n",
    "- Custom `MLflowTracker` wraps MLflow API\n",
    "- Logs per-fold, per-hyperparameter combination\n",
    "- Automatic experiment organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLOps: MLflow Integration\n",
    "print(\"MLOps: MLflow Experiment Tracking\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    import mlflow\n",
    "    \n",
    "    # Check for MLflow tracking directory\n",
    "    mlflow_dir = project_root / \"mlruns\"\n",
    "    \n",
    "    if mlflow_dir.exists():\n",
    "        print(f\"✓ MLflow tracking directory found: {mlflow_dir}\")\n",
    "        \n",
    "        # List experiments\n",
    "        try:\n",
    "            mlflow.set_tracking_uri(str(mlflow_dir))\n",
    "            experiments = mlflow.search_experiments()\n",
    "            print(f\"\\nFound {len(experiments)} experiments:\")\n",
    "            \n",
    "            for exp in experiments[:5]:  # Show first 5\n",
    "                print(f\"  - {exp.name}: {exp.experiment_id}\")\n",
    "                \n",
    "                # Get runs for this experiment\n",
    "                runs = mlflow.search_runs(experiment_ids=[exp.experiment_id], max_results=3)\n",
    "                if not runs.empty:\n",
    "                    print(f\"    Runs: {len(runs)} (showing 3)\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Could not query MLflow: {e}\")\n",
    "    else:\n",
    "        print(\"⚠ MLflow tracking directory not found.\")\n",
    "        print(\"  MLflow will be initialized on first training run.\")\n",
    "    \n",
    "    print(\"\\nMLflow Usage:\")\n",
    "    print(\"```python\")\n",
    "    print(\"from lib.mlops.mlflow_tracker import create_mlflow_tracker\")\n",
    "    print(\"\")\n",
    "    print(\"tracker = create_mlflow_tracker(\")\n",
    "    print(\"    experiment_name='fvc_binary_classifier',\")\n",
    "    print(\"    use_mlflow=True\")\n",
    "    print(\")\")\n",
    "    print(\"\")\n",
    "    print(\"tracker.log_config(config_dict)\")\n",
    "    print(\"tracker.log_metric('train_loss', 0.5, step=1)\")\n",
    "    print(\"tracker.log_artifact('model.pt')\")\n",
    "    print(\"```\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"⚠ MLflow not installed. Install with: pip install mlflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Experiment Tracker\n",
    "\n",
    "**RunConfig**: Dataclass for complete run configuration\n",
    "- Experiment metadata (run_id, experiment_name, tags)\n",
    "- Data config (splits, random seed)\n",
    "- Video config (num_frames, fixed_size, augmentation)\n",
    "- Training config (batch_size, learning_rate, epochs)\n",
    "- Model config (model_type, model_specific_config)\n",
    "\n",
    "**ExperimentTracker**: Tracks experiments, metrics, artifacts\n",
    "- JSONL metrics logging (append-only)\n",
    "- Config versioning with hashing\n",
    "- Checkpoint management\n",
    "- Resume capability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Analytics with DuckDB\n",
    "\n",
    "### Why DuckDB?\n",
    "\n",
    "**Benefits**:\n",
    "- **Fast Analytics**: SQL queries on Polars DataFrames\n",
    "- **Direct File Access**: Query Arrow/Parquet files directly\n",
    "- **Zero-Copy**: Efficient memory usage\n",
    "- **SQL Interface**: Familiar query language\n",
    "\n",
    "**Use Cases**:\n",
    "- Cross-stage data analysis\n",
    "- Feature correlation analysis\n",
    "- Training results aggregation\n",
    "- Performance comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analytics: DuckDB Integration\n",
    "print(\"Analytics: DuckDB Integration\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    from lib.utils.duckdb_analytics import DuckDBAnalytics\n",
    "    \n",
    "    # Initialize DuckDB\n",
    "    analytics = DuckDBAnalytics()\n",
    "    \n",
    "    # Register metadata files\n",
    "    if scaled_metadata.exists():\n",
    "        analytics.register_arrow(\"scaled_videos\", str(scaled_metadata))\n",
    "        print(\"✓ Registered scaled_videos table\")\n",
    "        \n",
    "        # Example query: Video statistics by label\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            label,\n",
    "            COUNT(*) as count,\n",
    "            AVG(duration) as avg_duration,\n",
    "            AVG(fps) as avg_fps\n",
    "        FROM scaled_videos\n",
    "        GROUP BY label\n",
    "        \"\"\"\n",
    "        \n",
    "        result = analytics.query(query)\n",
    "        print(\"\\nVideo Statistics by Label:\")\n",
    "        display(result)\n",
    "    \n",
    "    if features_metadata.exists():\n",
    "        analytics.register_arrow(\"features\", str(features_metadata))\n",
    "        print(\"✓ Registered features table\")\n",
    "        \n",
    "        # Example query: Feature correlation with label\n",
    "        # (This would require numeric label encoding)\n",
    "        print(\"\\n✓ Features table registered for analysis\")\n",
    "    \n",
    "    analytics.close()\n",
    "    \n",
    "    print(\"\\nDuckDB Usage:\")\n",
    "    print(\"```python\")\n",
    "    print(\"from lib.utils.duckdb_analytics import DuckDBAnalytics\")\n",
    "    print(\"\")\n",
    "    print(\"analytics = DuckDBAnalytics()\")\n",
    "    print(\"analytics.register_parquet('videos', 'data/scaled_videos/scaled_metadata.parquet')\")\n",
    "    print(\"result = analytics.query('SELECT * FROM videos WHERE label = \\\"real\\\"')\")\n",
    "    print(\"```\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"⚠ DuckDB not installed. Install with: pip install duckdb\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Error using DuckDB: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Airflow Orchestration\n",
    "\n",
    "### Pipeline Orchestration\n",
    "\n",
    "**Apache Airflow DAG**: `airflow/dags/fvc_pipeline_dag.py`\n",
    "\n",
    "**Stages as Tasks**:\n",
    "1. **Stage 1 Task**: Video augmentation\n",
    "2. **Stage 2 Task**: Feature extraction (depends on Stage 1)\n",
    "3. **Stage 3 Task**: Video scaling (depends on Stage 1)\n",
    "4. **Stage 4 Task**: Scaled feature extraction (depends on Stage 3)\n",
    "5. **Stage 5 Task**: Model training (depends on Stages 2, 3, 4)\n",
    "\n",
    "**Benefits**:\n",
    "- **Dependency Management**: Automatic task ordering\n",
    "- **Retry Logic**: Automatic retries on failure\n",
    "- **Monitoring**: Web UI for pipeline status\n",
    "- **Scheduling**: Cron-based scheduling\n",
    "- **Parallelization**: Parallel stage execution where possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Airflow: Pipeline Orchestration\n",
    "print(\"Airflow: Pipeline Orchestration\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "airflow_dag = project_root / \"airflow\" / \"dags\" / \"fvc_pipeline_dag.py\"\n",
    "\n",
    "if airflow_dag.exists():\n",
    "    print(f\"✓ Airflow DAG found: {airflow_dag}\")\n",
    "    \n",
    "    # Read and show DAG structure\n",
    "    with open(airflow_dag, 'r') as f:\n",
    "        dag_content = f.read()\n",
    "        \n",
    "    # Extract task definitions (simplified)\n",
    "    if \"stage1_task\" in dag_content:\n",
    "        print(\"\\nDAG Tasks:\")\n",
    "        print(\"  1. stage1_task: Video Augmentation\")\n",
    "        print(\"  2. stage2_task: Feature Extraction (→ stage1_task)\")\n",
    "        print(\"  3. stage3_task: Video Scaling (→ stage1_task)\")\n",
    "        print(\"  4. stage4_task: Scaled Features (→ stage3_task)\")\n",
    "        print(\"  5. stage5_task: Model Training (→ stage2_task, stage3_task, stage4_task)\")\n",
    "        \n",
    "    print(\"\\nAirflow Usage:\")\n",
    "    print(\"1. Start Airflow webserver: `airflow webserver --port 8080`\")\n",
    "    print(\"2. Start Airflow scheduler: `airflow scheduler`\")\n",
    "    print(\"3. Access UI: http://localhost:8080\")\n",
    "    print(\"4. Trigger DAG: `airflow dags trigger fvc_pipeline`\")\n",
    "else:\n",
    "    print(\"⚠ Airflow DAG not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Results & Insights\n",
    "\n",
    "### Model Performance Summary\n",
    "\n",
    "**Expected Performance** (typical deepfake detection):\n",
    "- **Baseline Models** (Logistic Regression, SVM): 60-75% accuracy\n",
    "- **Feature-based Models** (XGBoost with handcrafted features): 70-80% accuracy\n",
    "- **Deep Learning Models** (3D CNNs, Transformers): 80-90% accuracy\n",
    "- **Ensemble Models**: 85-95% accuracy\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. **Handcrafted Features Matter**: Complement deep learning features\n",
    "2. **Augmentation Critical**: 10x dataset size significantly improves performance\n",
    "3. **Temporal Modeling**: 3D CNNs and Transformers capture temporal patterns\n",
    "4. **Ensemble Works**: Combining multiple models improves robustness\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Hyperparameter Tuning**: Expand grid search\n",
    "2. **Architecture Search**: Try more model variants\n",
    "3. **Feature Engineering**: Explore additional handcrafted features\n",
    "4. **Ensemble Methods**: Combine predictions from multiple models\n",
    "5. **Deployment**: Production inference pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results Summary\n",
    "print(\"Results & Insights\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load training results if available\n",
    "stage5_dir = project_root / \"data\" / \"stage5\"\n",
    "\n",
    "if stage5_dir.exists():\n",
    "    results_summary = []\n",
    "    \n",
    "    for model_dir in stage5_dir.iterdir():\n",
    "        if not model_dir.is_dir():\n",
    "            continue\n",
    "            \n",
    "        model_name = model_dir.name\n",
    "        metrics_files = list(model_dir.rglob(\"metrics.json\"))\n",
    "        \n",
    "        if metrics_files:\n",
    "            # Load first metrics file\n",
    "            with open(metrics_files[0], 'r') as f:\n",
    "                metrics = json.load(f)\n",
    "            \n",
    "            results_summary.append({\n",
    "                \"model\": model_name,\n",
    "                \"test_accuracy\": metrics.get(\"test_accuracy\", 0),\n",
    "                \"test_f1\": metrics.get(\"test_f1\", 0),\n",
    "                \"test_auc\": metrics.get(\"test_auc\", 0),\n",
    "            })\n",
    "    \n",
    "    if results_summary:\n",
    "        results_df = pd.DataFrame(results_summary)\n",
    "        results_df = results_df.sort_values(\"test_accuracy\", ascending=False)\n",
    "        \n",
    "        print(\"\\nModel Performance Summary:\")\n",
    "        display(results_df.head(10))\n",
    "        \n",
    "        # Visualization\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        ax.barh(results_df[\"model\"][:10], results_df[\"test_accuracy\"][:10])\n",
    "        ax.set_xlabel(\"Test Accuracy\")\n",
    "        ax.set_title(\"Top 10 Models by Accuracy\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"⚠ No metrics files found.\")\n",
    "else:\n",
    "    print(\"⚠ No training results found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the complete end-to-end pipeline for deepfake video detection:\n",
    "\n",
    "1. **Data Extraction**: From ZIP archives to organized video datasets\n",
    "2. **Data Exploration**: Understanding dataset characteristics\n",
    "3. **Augmentation**: Generating diverse training samples\n",
    "4. **Feature Engineering**: Handcrafted features for interpretability\n",
    "5. **Video Preprocessing**: Scaling for consistent model inputs\n",
    "6. **Model Training**: 23 different architectures with hyperparameter tuning\n",
    "7. **MLOps**: Experiment tracking, versioning, orchestration\n",
    "8. **Analytics**: Fast SQL queries on pipeline data\n",
    "\n",
    "**Key Technologies**:\n",
    "- PyTorch, torchvision, timm (Deep Learning)\n",
    "- Polars, PyArrow, DuckDB (Data Processing)\n",
    "- MLflow (Experiment Tracking)\n",
    "- Airflow (Orchestration)\n",
    "- PyAV, OpenCV (Video Processing)\n",
    "\n",
    "**Production-Ready Features**:\n",
    "- Memory-efficient processing (chunked, frame-by-frame)\n",
    "- Reproducible experiments (deterministic seeds, versioning)\n",
    "- Scalable architecture (distributed training ready)\n",
    "- Comprehensive monitoring (MLflow, logging)\n",
    "\n",
    "For individual model details, see the model-specific notebooks (5a-5u)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
